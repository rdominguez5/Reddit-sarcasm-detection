{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_union, make_pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.preprocessing import RobustScaler, Normalizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from nltk import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.externals import joblib\n",
    "from CustomTransformers import ItemSelector, ArrayCaster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load corpora\n",
    "sarcastic = pd.read_csv('../data/sarcastic.csv') \n",
    "non_sarcastic = pd.read_csv('../data/non_sarcastic.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "                                                    #Pre-processing\n",
    "       \n",
    "#Remove '/s' annotation\n",
    "sarcastic['body'] = sarcastic['body'].apply(lambda x: x.replace('/s', ''))\n",
    "\n",
    "#Construct dataset\n",
    "dataset = pd.DataFrame()\n",
    "dataset['comment'] = sarcastic['body'].append(non_sarcastic['body'])\n",
    "dataset['subreddit'] = pd.Series(sarcastic['subreddit']).append(pd.Series(non_sarcastic['subreddit']))\n",
    "dataset['score'] = pd.Series(sarcastic['score']).append(pd.Series(non_sarcastic['score']))\n",
    "sarcastic_labels = pd.Series(np.full(len(sarcastic), 1.0))\n",
    "non_sarcastic_labels = pd.Series(np.full(len(non_sarcastic), 0.0))\n",
    "dataset['target'] = sarcastic_labels.append(non_sarcastic_labels)\n",
    "\n",
    "#Transform scores\n",
    "dataset['score'] = dataset['score'].apply(lambda x: abs(x))\n",
    "\n",
    "#Remove deleted\n",
    "dataset = dataset[dataset.comment != '[deleted]']\n",
    "\n",
    "#Reset index\n",
    "dataset = dataset.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rodrack\\Anaconda\\envs\\snowflakes\\lib\\site-packages\\ipykernel\\__main__.py:13: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "                                                #Pre-processing\n",
    "#Remove non-English comments\n",
    "\n",
    "def calculate_ratios(text):\n",
    "    languages_ratios = {}\n",
    "\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens]\n",
    "\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "\n",
    "        languages_ratios[language] = len(common_elements) \n",
    "\n",
    "    return languages_ratios\n",
    "\n",
    "\n",
    "def is_English(text):\n",
    "    ratios = calculate_ratios(text)\n",
    "\n",
    "    most_rated_language = max(ratios, key=ratios.get)\n",
    "\n",
    "    return True if most_rated_language == 'english' else False\n",
    "\n",
    "dataset = dataset[dataset.apply(lambda x: is_English(x['comment']), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert numerical columns to numbers\n",
    "dataset.apply(lambda x: pd.to_numeric(x, errors='ignore'))\n",
    "\n",
    "#Split data (50%/30%/20%)\n",
    "msk = np.random.rand(len(dataset)) < 0.5\n",
    "training_data = dataset[msk]\n",
    "test_data = dataset[~msk]\n",
    "msk = np.random.rand(len(test_data)) < 0.3\n",
    "validation_data = test_data[msk]\n",
    "test_data = test_data[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "                                                #Custom Transformers\n",
    "\n",
    "# class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "#     def __init__(self, key):\n",
    "#         self.key = key\n",
    "\n",
    "#     def fit(self, x, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, data_dict):\n",
    "#         return data_dict[self.key]\n",
    "    \n",
    "# class ArrayCaster(BaseEstimator, TransformerMixin):\n",
    "#     def fit(self, x, y=None):\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, data):\n",
    "#         print data.shape\n",
    "#         print np.transpose(np.matrix(data)).shape\n",
    "#         return np.transpose(np.matrix(data))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#algorithm = MultinomialNB()\n",
    "#algorithm = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "#algorithm = LogisticRegression(C=1e5)\n",
    "\n",
    "n = 2\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "ngram_count_vectorizer = Pipeline([\n",
    "        ('extract', ItemSelector(key='comment')),\n",
    "        ('ngram_tf_idf', TfidfVectorizer(ngram_range=(1, n), min_df=1))\n",
    "    ])\n",
    "\n",
    "subreddit_vectorizer = Pipeline([\n",
    "        ('extract', ItemSelector(key='subreddit')),\n",
    "        ('ngram_tf_idf', TfidfVectorizer(ngram_range=(1, n), min_df=1))\n",
    "    ])\n",
    "\n",
    "#N-grams and subreddit\n",
    "pipeline2 = Pipeline([\n",
    "  ('features', FeatureUnion([                   \n",
    "    ('ngrams', ngram_count_vectorizer),\n",
    "    ('subreddits', subreddit_vectorizer),\n",
    "  ])),\n",
    "  ('classifier', classifier)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "text_clf = pipeline2.fit(training_data, training_data.target)\n",
    "joblib.dump(text_clf, 'model.pkl') \n",
    "# text_clf.fit(training_data, training_data.target)\n",
    "# predicted = text_clf.predict(validation_data)\n",
    "# predicted2 = text_clf.predict(jere)\n",
    "\n",
    "# predicted2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25799L,)\n",
      "(25799L, 1L)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "no supported conversion for types: (dtype('float64'), dtype('O'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9bb66769e503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[1;31m# ])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mtext_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'filename.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Rodrack\\Anaconda\\envs\\snowflakes\\lib\\site-packages\\sklearn\\pipeline.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Rodrack\\Anaconda\\envs\\snowflakes\\lib\\site-packages\\sklearn\\pipeline.pyc\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit_transform\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Rodrack\\Anaconda\\envs\\snowflakes\\lib\\site-packages\\sklearn\\pipeline.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_transformer_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mXs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0mXs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m             \u001b[0mXs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Rodrack\\Anaconda\\envs\\snowflakes\\lib\\site-packages\\scipy\\sparse\\construct.pyc\u001b[0m in \u001b[0;36mhstack\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \"\"\"\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Rodrack\\Anaconda\\envs\\snowflakes\\lib\\site-packages\\scipy\\sparse\\construct.pyc\u001b[0m in \u001b[0;36mbmat\u001b[0;34m(blocks, format, dtype)\u001b[0m\n\u001b[1;32m    590\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mall_dtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblock_mask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mall_dtypes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mall_dtypes\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0mrow_offsets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbrow_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Rodrack\\Anaconda\\envs\\snowflakes\\lib\\site-packages\\scipy\\sparse\\sputils.pyc\u001b[0m in \u001b[0;36mupcast\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'no supported conversion for types: %r'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: no supported conversion for types: (dtype('float64'), dtype('O'))"
     ]
    }
   ],
   "source": [
    "#                                                 #Model training\n",
    "\n",
    "# #algorithm = MultinomialNB()\n",
    "# #algorithm = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42)\n",
    "# #algorithm = LogisticRegression(C=1e5)\n",
    "\n",
    "n = 2\n",
    "classifier = MultinomialNB()\n",
    "\n",
    "ngram_count_vectorizer = Pipeline([\n",
    "        ('extract', ItemSelector(key='comment')),\n",
    "        ('ngram_tf_idf', TfidfVectorizer(ngram_range=(1, n), min_df=1))\n",
    "    ])\n",
    "\n",
    "# # pipeline0 = Pipeline([\n",
    "# #          ('ngram_tf_idf', TfidfVectorizer(ngram_range=(1, n), min_df=1)),\n",
    "# #          ('clf', classifier)\n",
    "# #     ])\n",
    "\n",
    "# # text_clf = pipeline0.fit(training_data.comment, training_data.target)\n",
    "# # predicted = text_clf.predict(validation_data.comment)\n",
    "# #np.mean(predicted == test_data.target) \n",
    "\n",
    "score_vectorizer = Pipeline([\n",
    "        ('extract', ItemSelector(key='score')),\n",
    "        ('caster', ArrayCaster()),\n",
    "        ('scale', Normalizer())\n",
    "    ])\n",
    "\n",
    "\n",
    "subreddit_vectorizer = Pipeline([\n",
    "        ('extract', ItemSelector(key='subreddit')),\n",
    "        ('caster', ArrayCaster())\n",
    "    ])\n",
    "\n",
    "#Transform scores\n",
    "\n",
    "\n",
    "#N-grams and score\n",
    "# pipeline1 = Pipeline([\n",
    "#   ('features', FeatureUnion([                   \n",
    "#     ('ngrams', ngram_count_vectorizer),\n",
    "#     ('normalized_scores', score_vectorizer),\n",
    "#   ])),\n",
    "#   ('classifier', classifier)\n",
    "# ])\n",
    "\n",
    "# # #N-grams and subreddit\n",
    "pipeline2 = Pipeline([\n",
    "  ('features', FeatureUnion([                   \n",
    "    ('ngrams', ngram_count_vectorizer),\n",
    "    ('subreddits', subreddit_vectorizer),\n",
    "  ])),\n",
    "  ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# #N-grams, score and subreddit\n",
    "# pipeline3 = Pipeline([\n",
    "#   ('features', FeatureUnion([                   \n",
    "#     ('ngrams', ngram_count_vectorizer),\n",
    "#     ('normalized_scores', score_vectorizer),\n",
    "#     ('subreddits', subreddit_vectorizer),\n",
    "#   ])),\n",
    "#   ('classifier', classifier)\n",
    "# ])\n",
    "\n",
    "text_clf = pipeline2.fit(training_data, training_data.target)\n",
    "joblib.dump(text_clf, 'filename.pkl') \n",
    "\n",
    "#predicted = text_clf.predict(validation_data)\n",
    "# np.mean(predicted == test_data.target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator =joblib.load(\"model.pkl\")\n",
    "jere = pd.DataFrame()\n",
    "jere['comment'] = pd.Series('jjrjerjeje')\n",
    "jere['subreddit'] = pd.Series('ForeverAlone')\n",
    "name_category = estimator.predict(jere)\n",
    "\n",
    "name_category[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72686433063791556"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                                #Results\n",
    "    \n",
    "target_names = ['sarcastic', 'non-sarcastic']\n",
    "#print(classification_report(validation_data.target, predicted, target_names=target_names))\n",
    "accuracy_score(validation_data.target, predicted)\n",
    "# tn, fp, fn, tp = confusion_matrix(validation_data.target, predicted).ravel()\n",
    "# tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:snowflakes]",
   "language": "python",
   "name": "conda-env-snowflakes-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
